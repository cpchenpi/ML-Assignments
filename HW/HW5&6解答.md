# HW5

#### 5.1 

  **试述将线性函数 $f(x) = w^\top x$ 用作神经元激活函数的缺陷** 

#### Sol.

（言之有理即可）
解：当激活函数为线性函数 $f(x) = w^\top x$时，每一层输出都是上层输入的线性函数，
无论神经网络有多少层，输出都是输入的线性组合，此时设置多层对逼近函数没有额外的贡献，神经网络实际仍为原始的感知机，无法处
理非线性问题。使用非线性激活函数增加了神经网络模型的非线性因素，使得神经网络可以
任意逼近任何非线性函数，可以应用到非线性模型中。

#### 2.
​	
**讨论 $\frac{\exp(x_i)}{\sum_{j=1}\exp(x_j)}$ 和 $\log\sum_{j=1}\exp(x_j)$ 的数值溢出问题**

#### Sol.

实数在计算机中以二进制表示，计算时非精确值。当数值过小会取0（下溢出）或数值过
大导致上溢出。在Softmax情境下的解决方案为：

令 $M=\max(x_i), i\in\{1,\cdots, C\}$ ，此时计算上述两值为，

$$
\frac{\exp(x_i)}{\sum_{j=1}\exp(x_j)}=\frac{\exp(x_i-M)}{\sum_{j=1}\exp(x_j-M)}
$$

$$
\log\sum_{j=1}\exp(x_j) = M +\log\sum_{j=1}\exp(x_j-M)
$$

此时 $\exp(x_i-M)\leq \exp(M-M)=1$ , 不会发生上溢出；

$\frac{1}{\sum_{j=1}\exp(x_j-M)}\geq\frac{1}{\sum_{j=1}\exp(M-M)}=\frac{1}{C}$, 不会发生下溢出。

#### 3. 
​	
**计算 $\frac{\exp(x_i)}{\sum_{j=1}\exp(x_j)}$ 和 $\log\frac{\exp(x_i)}{\sum_{j=1}\exp(x_j)}$ 关于向量 $\boldsymbol{x} = [x_1,\cdots,x_C]$ 的梯度**

#### Sol.

记 $f(x_i) = \frac{\exp(x_i)}{\sum_{j=1}\exp(x_j)}, g(x_i) = \log f(x_i)$,则有

$$
\frac{\partial f(x_i)}{x_i} = \frac{\exp(x_i)\sum_{j=1}\exp(x_j)-\exp(x_i)^2}{(\sum_{j=1}\exp(x_j))^2}=\frac{\exp(x_i)}{\sum_{j=1}\exp(x_j)}-\frac{\exp(x_i)^2}{(\sum_{j=1}\exp(x_j))^2} = f(x_i)(1-f(x_i))
$$

$$
\frac{\partial f(x_i)}{x_j} = -f(x_i)f(x_j), \forall j\neq i 
$$

$$
\implies \frac{\partial f(x_i)}{\partial \boldsymbol{x}}=f(x_i)[-f(x_1),\cdots,-f(x_{i-1}),1-f(x_i),-f(x_{i+1}),\cdots,-f(x_C)]
$$

$$
\frac{\partial g(x_i)}{\partial \boldsymbol{x}} = [-f(x_1),\cdots,-f(x_{i-1}),1-f(x_i),-f(x_{i+1}),\cdots,-f(x_C)]
$$

#### 4.

**考虑如下简单网络,假设激活函数为ReLU ,用平方损失 $\frac{1}{2}(y − \hat{y})^2$计算误差，请用BP算法更新一次所有参数 (学习率为1)，给出更新后的参数值(给出详细计算过程)，并计算给定输入值$x = (0.2, 0.3)$时初始时和更新后的输出值,检查参数更新是否降低了平方损失值.**

#### Sol:

注意激活函数为ReLU函数！

$$
ReLU(x) = \begin{cases}x, x\gt 0 \\\ 0, x \leq 0\end{cases},\qquad \frac{\partial ReLU(x)}{\partial x} = \begin{cases} 1, x\gt 0 \\\ 0, x \leq 0\end{cases}
$$

已知 $v_{11} = 0.6, v_{12} = 0.1, v_{21} = 0.2, v_{22} = 0.7, w_1 = 0.5, w_2 = 0.8$, 令 $\alpha_1,\alpha_2,\gamma$为结点1，2，3的输入，$\beta_1,\beta_2,\hat{y}$为对应输出，则

正向传播：

$$
\begin{gather}
\alpha_1 = v_{11}A+v_{21}B = 0.18 = \beta_1\\
\alpha_2 = v_{12}A+v_{22}B = 0.23 = \beta_2\\
\gamma = w_1\beta_1 + w_2\beta_2 = 0.274 = \hat{y}\\
E =\frac{1}{2}(y − \hat{y})^2 = 0.025538
\end{gather}
$$

逆传播（链式法则）：

$$
\begin{gather}
\frac{\partial E}{\partial v_{11}} = \frac{\partial E}{\partial \hat{y}}\frac{\partial \hat{y}}{\partial\gamma}\frac{\partial\gamma}{\partial\beta_1}\frac{\partial\beta_1}{\partial\alpha_1}\frac{\partial\alpha_1}{\partial v_{11}} = -0.0226\\
\frac{\partial E}{\partial v_{12}} = -0.03616\\
\frac{\partial E}{\partial v_{21}} = -0.0339\\
\frac{\partial E}{\partial v_{22}} = -0.05424\\
\frac{\partial E}{\partial w_1} = -0.04068\\
\frac{\partial E}{\partial w_2} = -0.05198
\end{gather}
$$

更新权重with $\eta = 1$:

$$
\begin{gather}
v_{11}' = v_{11}-\eta \frac{\partial E}{\partial v_{11}} = 0.6226\\
v_{12}' = v_{12}-\eta \frac{\partial E}{\partial v_{12}} = 0.13616\\
v_{21}' = 0.2339\\
v_{22}' = 0.75424\\
w_1' = 0.54068\\
w_2' = 0.85198
\end{gather}
$$

为验证是否降低平方损失，进行第二次正向传播，得到平方损失

$$
E' = 0.015976 \lt 0.025538
$$

参数更新确实降低了平方损失。

# HW6

#### 1.

**试讨论LDA与SVM在何种条件下等价.**

#### Sol.

（言之有理即可）
线性判别分析能够解决 n 分类问题，而线性核 SVM 只能解决二分类问题。当线性判
别分析的投影向量和线性核SVM的超平面向量垂直的时候，SVM 的最大间隔就是线性判别
分析所要求的异类投影点间距，同时在这种情况下，线性判别分析的同类样例的投影点也会
被这个超平面所划分在一起，使其间隔较小。所以:

- 线性判别分析求解出来的投影向量和线性核 SVM 求解出来的超平面向量垂直，

- 数据集只有两类，

- 数据集线性可分时，SVM 和 LDA等价。

#### 2.

**试析SVM对噪声敏感的原因**

#### Sol.

（言之有理即可）
- SVM 的基本形态是一个硬间隔分类器，它要求所有样本都满足硬间隔约束，因此噪声很容易影响 SVM 的学习。

- 存在噪声时，SVM 容易受噪声信息的影响，将训练得到的超平面向两个类间靠拢，导致训练的泛化能力降低，尤其是当噪声成为支持向量时，会直接影响整个超平面。

- 当 SVM 推广到到使用核函数时，会得到一个更复杂的模型，此时噪声也会一并被映射到更高维的特征，可能会对训练造成更意想不到的结果。

综上， SVM 对噪声敏感。

#### 3.

**试使用核技巧推广对率回归产生“核对率回归”**

#### Sol.

考虑对率回归的目标函数

$$
\begin{gather}
\ell(\boldsymbol{\beta}) = \sum_{i=1}(-y_i \boldsymbol{\beta}^\top \hat{x}_i+ \log(1+\exp(\boldsymbol{\beta}^\top \hat{x}_i))\\
F = \ell(\boldsymbol{\beta})+\lambda\||\boldsymbol{\beta}\||
\end{gather}
$$

由表示定理，$\boldsymbol{\beta} = \sum_{i=1}\alpha_i\phi(\boldsymbol{x}_i),代入得

$$
\begin{gather}
F = \sum_i(-y_i\sum_{j=1}\alpha_j\phi(x_i)\phi(x_j)+\log(1+\exp(\sum_{j=1}\alpha_j\phi(x_i)\phi(x_j))))+\lambda\||\sum_{j}\alpha_j\phi(x_j)\||^2\\
=\sum_i(-y_i\sum_j\alpha_j\kappa(x_i,x_j)+\log(1+\exp(\sum_j\alpha_j\kappa(x_i,x_j)))+\lambda\sum_j\alpha_i\alpha_j\kappa(x_i,x_j)
\end{gather}
$$

#### 4.

**支持向量回归的对偶问题如下，**

$$
\begin{gather}
\max_{\alpha,\hat{\alpha}} g(\alpha,\hat{\alpha}) = -\frac{1}{2}\sum_i\sum_j(\alpha_i-\hat{\alpha}_i)(\alpha_j-\hat{\alpha}_j)\kappa(x_i,x_j)+\sum_i(y_i(\hat{\alpha}_i-\alpha_i)-\epsilon(\hat{\alpha}_i-\alpha_i))\\
s.t.\quad C\succeq \alpha,\hat{\alpha}\succeq 0\quad and \quad \sum_i(\alpha_i-\hat{\alpha_i})=0
\end{gather}
$$


**请将该问题转化为类似于如下的标准形式**

$$
\begin{gather}
\max_\alpha g(\alpha) = \alpha^\top v-\frac{1}{2}\alpha^\top K\alpha\\
s.t. C\succeq \alpha \succeq 0\quad and \quad \alpha^\top u = 0
\end{gather}
$$


#### Sol.

令 $\alpha = [\alpha_1,\cdots, \alpha_m]^\top$ , $ \hat{\alpha} = [\hat{\alpha} _{1}, \cdots , \hat{\alpha}_m] $ , $ y = [y_1, \cdots , y_m]^\top$, $ K _{ij} = \kappa (x_{i}, x_{j})$ , $\varepsilon^* = [\epsilon,\cdots,\epsilon]^\top$, $\alpha^* = [\alpha,\hat{\alpha}]$ , $v = [-y-\varepsilon^*, y-\varepsilon^*]^\top$, $ K^* = \left[ \begin{array} K  -K\\ -K  K\end{array} \right]$

(Updating...)



