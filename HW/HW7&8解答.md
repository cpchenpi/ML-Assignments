### 8.2
对于 $0 / 1$ 损失函数来说, 指数损失函数并非仅有的一致替代函数. 考虑式 $(8.5)$, 试证明: 任意损失函数 $\ell(-f(\boldsymbol{x}) H(\boldsymbol{x}))$, 若对于 $f(\boldsymbol{x})H(\boldsymbol{x})$ 在区间 $[-\infty, \delta](\delta>0)$ 上单调递减, 则 $\ell$ 是 $0 / 1$ 损失函数的一致替代函数.
### Sol.
对损失函数关于 ${H}({x})$ 求导:

$$
\frac{\partial \mathbb{E}[l(-f(x) H(x)) \mid x]}{\partial H(x)}=-E f(x) l^{\prime}(-f(x) H(x))=-l^{\prime}(-H(x)) P(f(x)=1 \mid x)+l^{\prime}(H(x)) P(f(x)=-1 \mid x)
$$

令上式为 0 , 得到损失函数最小时满足 $\frac{l^{\prime}(H(x))}{l^{\prime}(-H(x))}=\frac{P(f(x)=1 \mid x)}{P(f(x)=-1 \mid x)}$
由于 $l(-f(x) H(x))$ 关于 ${f}({x}) {H}({x})$ 递减, 且为凸函数, 故 ${l}^{\prime}$ 为增函数。从而 ${P}({f}({x})=1 \mid {x})>{P}({f}({x})=-1 \mid {x})$ 时, ratio 大于 1 , 左侧 ${H}({x})$ 应取值为 1. 类似分析, 当 ratio<1 时, ${H}({x})$ 取值为 -1 。综上, $\text{sign}(H(x))=$ $\arg \max _{y \in\{-1,1\}} P(f(x)=y \mid x)$, 故达到贝叶斯最优错误率, $l$ 是一致替代函数。

备注: 要在给定 $x$ 的情况对 $H(x)$ 求导才有意义，书本有误，参考 [原论文4.1节 lemma 1](asset/adaboost.pdf)
### 8.6 
试析 Bagging 通常为何难以提升朴素贝叶斯分类器的性能.
### Sol. 

Bagging作为一种常见的集成方法，通常难以提升朴素贝叶斯分类器的性能。这一局限主要源于朴素贝叶斯分类器固有的低方差特性。由于集成方法如Bagging的目的是减少方差，因此在应用于本就表现出低方差的分类器时，其效果不太明显。因此，将朴素贝叶斯与其他分类器混合使用或应用集成方法，并不能显著提高分类器的准确性。

### 8.8
MultiBoosting 算法 [Webb, 2000] 将 AdaBoost 作为 Bagging 的基学习器, Iterative Bagging 算法 [Breiman, 2001b] 则是将 Bagging 作为 AdaBoost 的基学习器. 试比较二者的优缺点.
### Sol.
MultiBoosting 和 Iterative Bagging 都是机器学习中的集成方法，它们通过结合多个弱学习器来创建更强的学习器。然而，它们的方法和效果有显著差异。

**MultiBoosting**：
- 在Bagging框架中使用AdaBoost作为基学习器。
- AdaBoost关注于调整训练实例的权重，对后续迭代中被误分类的实例给予更多重视。
- 这种方法可能导致更准确的模型，因为它优化了单个模型的优势并最小化了缺点。
- 然而，如果不小心管理，也可能导致过拟合。

**Iterative Bagging**：
- 在AdaBoost框架中使用Bagging作为基学习器。
- Bagging涉及创建多个模型（每个模型在训练数据的不同样本上训练），然后平均它们的预测。
- 这种方法倾向于更有效地降低方差，并且对于不稳定的分类器特别有益。
- 在单个模型容易过拟合的情况下，Bagging可以帮助解决这个问题。

MultiBoosting 更适用于提高在困难案例上的准确性的情况，而Iterative Bagging是在处理高方差或容易过拟合的模型时的更好选择。

### 1 
给定任意的两个相同长度向量 $x, y$ ，其余弦距离为 $1-\frac{x^{\top} y}{|x||y|}$, 证明余弦距离不满足传递性，而余弦夹角 $\arccos \left(\frac{x^{\top} y}{|x||y|}\right)$ 满足
### Sol.
余弦距离定义为 $1-\frac{x^{\top} y}{|x||y|}$，其中 $x^{\top} y$ 是向量 $x$ 和 $y$ 的点积， $|x|$ 和 $|y|$ 是这两个向量的模。为了证明余弦距离不满足传递性，我们需要证明存在向量 $x, y, z$ 使得以下不等式不成立：

$$
d(x, y) + d(y, z) \geq d(x, z)
$$

具体地，考虑三个单位向量 $x, y, z$，上式可以化简为

$$
< y - x, y - z > \geq 0
$$

取 $x = (-1,0), y= (0,1),z = (\frac{\sqrt{2}}{2}, \frac{\sqrt{2}}{2})$得出反例.


证明余弦夹角满足三角不等式：
**解法1**：
即证明 $\arccos \left(\frac{\boldsymbol{x}^T \boldsymbol{y}}{|\boldsymbol{x}||\boldsymbol{y}|}\right) \leq \arccos \left(\frac{\boldsymbol{x}^T \boldsymbol{z}}{|\boldsymbol{x}||\boldsymbol{z}|}\right)+\arccos \left(\frac{\boldsymbol{y}^T \boldsymbol{z}}{|\boldsymbol{y}||\boldsymbol{z}|}\right)$设 $\langle x, y\rangle=\gamma,\langle y, z\rangle=\beta,\langle x, z\rangle=\alpha, 0 \leq \alpha, \beta, \gamma \leq \pi$, 即证明

由三面角公式 $\cos \alpha=\cos \beta \cos \gamma+\sin \beta \sin \gamma \cos \theta$ (其中 $\theta$ 为二面角)于是知 $\cos \alpha \leq \cos \beta \cos \gamma+\sin \beta \sin \gamma=\cos (\gamma-\beta)$

若 $\beta \geq \gamma$, 则 $\gamma \leq \beta \leq \beta+\alpha$ 成立;
若 $\beta<\gamma$, 则由 $\cos \alpha \leq \cos (\gamma-\beta)$ 知 $\alpha \geq \gamma-\beta$, 即 $\gamma \leq \alpha+\beta$ 成立;故总是有 $\gamma \leq \alpha+\beta$ 成立, 证毕。

**解法2**：
 即证明: $\arccos x^T y \leq \arccos x^T z+\arccos z^T y$. 定义域为 $[-1,1]$等式两边取cos:

$$
x^T y \geq x^T z z^T y-\sqrt{x^T\left(I-z z^T\right) x} \sqrt{y^T\left(I-z z^T\right) y} .
$$

即证

$$
x^T\left(z z^T-I\right) y \leq \sqrt{x^T\left(I-z z^T\right) x} \sqrt{y^T\left(I-z z^T\right) y}
$$

由 Cauthy-Schwarz定理，上式成立，因此余弦央角满足三角不等式.


### 2 
证明k-means算法的收敛性
### Sol.
kmeans每轮迭代loss单调递减且下界为0。


### 3  
在k-means算法中替换欧式距离为其他任意的度量，请问 “聚类簇" 中心如何计算?
### Sol.

$$ 
\min \sum_{c = 1}^k \sum_{x \in c_c} f\left(x, \mu_c\right) 
$$ 

其中 $f\left(x, \mu_c\right)$ 度量了x. $\mu_c$ 间的距离。则更新 $\mu_c$ 的策略的目的是对每个c，使得 $\sum_{x \in c} f(x, \mu_c)$ 达到最小。 




