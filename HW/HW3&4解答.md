# HW3

#### 1. 

​	**[课本习题 3.2] 试证明, 对于参数 $w$, 对率回归的目标函数 (3.18) 是非凸的, 但其对数似然函数 (3.27) 是凸的。**



#### Sol.

$$
\begin{gathered}
y=\frac{1}{1+\mathrm{e}^{-\boldsymbol{w}^{\top} \boldsymbol{x}+b}} \\
\ell(\boldsymbol{\beta})=\sum_{i=1}^m\left(-y_i \boldsymbol{\beta}^{\top} \hat{\boldsymbol{x}}_i+\ln \left(1+e^{\boldsymbol{\beta}^{\top} \hat{\boldsymbol{x}}_i}\right)\right)
\end{gathered}
$$

证明. 需要注意, 标量函数先对向量变量的转置求导, 再对向量变量求导, 得到的是矩阵, 一些同学求成了标量
- 方法不唯一, 但是需要注意符号书写清晰
  
$$
\begin{gathered}
\frac{\partial y}{\partial \boldsymbol{w}}=\frac{\boldsymbol{x} e^{-\left(\boldsymbol{w}^{\top} \boldsymbol{x}+b\right)}}{\left(1+e^{-\left(\boldsymbol{w}^{\top} \boldsymbol{x}+b\right)}\right)^2}=\boldsymbol{x} y(1-y) \\
\frac{\partial^2 y}{\partial \boldsymbol{w}^{\top} \partial \boldsymbol{w}}=\frac{\partial}{\partial \boldsymbol{w}^{\top}} \frac{\partial y}{\partial \boldsymbol{w}}=\frac{\partial y}{\partial \boldsymbol{w}^{\top}} \boldsymbol{x}(1-y)-\frac{\partial y}{\partial \boldsymbol{w}^{\top}} \boldsymbol{x} y=\boldsymbol{x}^{\top} \boldsymbol{x} y(1-2 y)(1-y)
\end{gathered}
$$

$\boldsymbol{x}^{\top} \boldsymbol{x} \geq 0$ 恒成立, 当 $0.5\lt y\lt 1$ 时, $y(1-2 y)(1-y)\lt 0$, 此时 $\frac{\partial^2 y}{\partial \boldsymbol{w} \partial \boldsymbol{w}}\lt 0$, 因此函数 (3.18) 非凸。

$$
\frac{\partial \ell}{\partial \boldsymbol{\beta}}=\sum_{i=1}^m\left(-y_i \widehat{\boldsymbol{x}}_i+\frac{1}{1+\exp{\boldsymbol{\beta}^{\top}\widehat{x}_i}} \widehat{\boldsymbol{x}}_i \exp{\boldsymbol{\beta}^{\top} \widehat{\boldsymbol{x}}_i}\right) $$

$$
\frac{\partial^2 \ell}{\partial \boldsymbol{\beta}^{\top} \partial \boldsymbol{\beta}}=\frac{\partial}{\partial \boldsymbol{\beta}^{\top}} \frac{\partial \ell}{\partial \boldsymbol{\beta}}=\frac{\partial}{\partial \boldsymbol{\beta}^{\top}}\sum_{i=1}^m (-y _i \widehat{\boldsymbol{x}} _i+\frac{1}{1+\exp{\boldsymbol{\beta}^{\top}}\widehat{\boldsymbol{x}} _i} \widehat{\boldsymbol{x}} _i \exp(\boldsymbol{\beta}^{\top}\widehat{\boldsymbol{x}} _{i} )) = \sum _{i=1}^m \frac{\exp(\boldsymbol{\beta}^{\top}\widehat{\boldsymbol{x}} _{i})}{\left(1+\exp(\boldsymbol{\beta}^{\top} \widehat{\boldsymbol{x}} _{i})\right)^2}\widehat{\boldsymbol{x}} _{i}^{\top} \widehat{ \boldsymbol{x}} _{i}
$$

由于 $\widehat{\boldsymbol{x}}_i{ }^{\top} \widehat{\boldsymbol{x}}_i \geq 0$ 且 $\frac{e^{\beta^{\top} \hat{\boldsymbol{x}}_i}}{\left(1+e^{\beta^{\top} \hat{\boldsymbol{x}}_i}\right)^2} \geq 0$, 因此函数 (3.27) 为凸函数。



#### 2.
​	**[课本习题 3.7] 令码长为 9，类别数为 4，试给出海明距离意义下理论最优的 ECOC 二元码并证明之。**

#### Sol.

|       | $f_1$ | $f_2$ | $f_3$ | $f_4$ | $f_5$ | $f_6$ | $f_7$ | $f_8$        | $f_9$        |
| :---- | :---- | :---- | :---- | :---- | :---- | :---- | :---- | :----------- | :----------- |
| $C_1$ | 1     | 1     | 1     | 1     | 1     | 1     | 1     | $\mathrm{x}$ | $\mathrm{x}$ |
| $C_2$ | 0     | 0     | 0     | 0     | 1     | 1     | 1     | $\mathrm{x}$ | $\mathrm{x}$ |
| $C_3$ | 0     | 0     | 1     | 1     | 0     | 0     | 1     | $\mathrm{x}$ | $\mathrm{x}$ |
| $C_4$ | 0     | 1     | 0     | 1     | 0     | 1     | 0     | $\mathrm{x}$ | $\mathrm{x}$ |

#### 3. 
​	**在 LDA 多分类情形下, 试计算类间散度矩阵 $S_b$ 的秩, 并证明**

#### Sol.

$$
\boldsymbol{S}_b=\sum_c m_c\left(\boldsymbol{\mu}_c-\boldsymbol{\mu}\right)\left(\boldsymbol{\mu}_c-\boldsymbol{\mu}\right)^{\top}=\boldsymbol{A} \boldsymbol{M} \boldsymbol{A}^{\top}
$$

其中

$$
\boldsymbol{A}=\left(\begin{array}{llll}
\boldsymbol{\mu}_1-\boldsymbol{\mu} & \boldsymbol{\mu}_2-\boldsymbol{\mu} & \cdots & \boldsymbol{\mu}_N-\boldsymbol{\mu}
\end{array}\right), \quad \boldsymbol{M}=\text{diag}\left(m_1, m_2, \cdots, m_N\right) .
$$

接着, 可以得到

$$
\text{rank} \boldsymbol{S}_b=\text{rank}\left(\boldsymbol{A} \boldsymbol{M A}^{\top}\right)=\text{rank}\left(\left(\boldsymbol{A} \boldsymbol{M}^{\frac{1}{2}}\right)\left(\boldsymbol{A} \boldsymbol{M}^{\frac{1}{2}}\right)^{\top}\right)=\text{rank}\left(\boldsymbol{A} \boldsymbol{M}^{\frac{1}{2}}\right)=\text{rank} \boldsymbol{A}
$$

因为 $\sum_c m_i\left(\boldsymbol{\mu}_i-\boldsymbol{\mu}\right)=\mathbf{0}$, 所以 $\text{rank} \boldsymbol{S}_b=\text{rank} \boldsymbol{A} \leq \min(N-1,D)$. 

其中 D为数据点维度

#### 4 

​	**给出公式 (3.45) 的推导公式**

#### Sol.

书本此处 $W$是矩阵，不是向量，显然 $\lambda$表示有误，

$\max \text{tr}\left(W^{\top} S_b W\right) s.t. \text{tr}\left(W^{\top} S_W W\right)=1$
同样也等价子如下目标:

$$
\max \text{tr}\left(W^{\top} S_bW\right) \text { s.t. } W^{\top} S_W W=1
$$

利用 Lagrange乘子法: $L(W, \Lambda)=-\text{tr}\left[W^{\top} S_bW\right]+\text{tr}\left[\Lambda^{\top}\left(W^{\top} S_W W-I\right)\right]$

$$
\begin{aligned}
& \frac{\partial L(W, \Lambda)}{\partial W}=-2 S_b W+2 S_W W \Lambda=0 \\
& \therefore S_b W=S_W W \Lambda \text {. } \\
&
\end{aligned}
$$




#### 5. 
​	**证明 $X\left(X^{\top} X\right)^{-1} X^{\top}$ 是投影矩阵, 并对线性回归模型从投影角度解释。**



证明. 令 $P=X\left(X^{\top} X\right)^{-1} X^{\top}$, 那么

$$
P^{\top}=\left(X\left(X^{\top} X\right)^{-1} X^{\top}\right)^{\top}=X\left(X\left(X^{\top} X\right)^{-1}\right)^{\top}=X\left(X^{\top} X\right)^{-1} X^{\top}=P
$$

因此 $P$ 是一个对称矩阵, 又因为

$$
P^2=X\left(X^{\top} X\right)^{-1} X^{\top} X\left(X^{\top} X\right)^{-1} X^{\top}=X\left(X^{\top} X\right)^{-1}\left(X^{\top} X\right)\left(X^{\top} X\right)^{-1} X^{\top}=X\left(X^{\top} X\right)^{-1} X^{\top}=P
$$

因此 $P$ 是一个幕等矩阵, 所以 $P$ 是一个投影矩阵。
解释. 线性回归模型: $\hat{\boldsymbol{y}}=X^{\top}\left(X^{\top} X\right)^{-1} X^{\top} \boldsymbol{y}$ 。可以发现, $\hat{\boldsymbol{y}}$ 其实是 $\boldsymbol{y}$ 在线性空间的投影。



# HW4
#### 1. 
​	**[课本习题 4.1] 试证明对于不含冲突数据（即特征向量完全相同但标记不同）的训练集, 必存在与训练集一致 (即训练误差为 0 ) 的决策树。**

证明. (反证法) 假设不存在与训练集一致的决策树, 那么训练集训练得到的决策树必然含有冲突数据, 这与假设矛盾, 因此必然存在与训练集一致决策树。
#### 2.
​	**[课本习题 4.9] 试将 4.4.2 节对缺失值的处理机制推广到基尼指数的计算中去。解.**

$$
\begin{aligned}
\text{Gini}(D, a) & =\rho \times \text{Gini}\_\text{index}(\tilde{D}, a) \\
& =\rho \times \sum _{v=1}^V \tilde{r} _v \text{Gini}\left(\tilde{D}^v\right) \\
& =\rho \times \sum _{v=1}^V \tilde{r} _v\left(1-\sum _{i=1}^k \tilde{p} _k^2\right)
\end{aligned}
$$

#### 3.
​	**假设离散随机变量 $X \in\{1, \ldots, K\}$, 其取值为 $k$ 的概率 $P(X=k)=p_k$, 其摘为 $H(p)=-\sum_k p_k \log _2 p_k$ ，试用拉格朗日乘子法证明摘最大分布为均匀分布。**

证明.

$$
\begin{gathered}
L(p, \lambda)=-\sum _{i=1}^k p_i \log _2 p _i+\lambda\left(\sum _{i=1}^k p _i-1\right) \\
\frac{\partial L}{\partial p _i}=-\log _2 p _i-\frac{1}{\ln 2}+\lambda=0 \Longrightarrow p _1=p _2=\cdots=p _k=2^{\lambda-\frac{1}{\ln 2}} \\
\frac{\partial L}{\partial \lambda}=\sum _{i=1}^k p _i-1=0 \Longrightarrow p _1=p _2=\cdots=p _k=\frac{1}{k}
\end{gathered}
$$
#### 4.
​	**下表表示的二分类数据集，具有三个属性 A、B、C，样本标记为两类“+”, “-”。请运用学过的知识完成如下问题:**

|                  实例                   | $\mathrm{A}$  | $\mathrm{B}$  | $\mathrm{C}$ | 类别 |
| :-------------------------------------: | :-----------: | :-----------: | :----------: | :--: |
|                    1                    | $\mathrm{~T}$ | $\mathrm{~T}$ |     1.0      |  +   |
|                    2                    | $\mathrm{~T}$ | $\mathrm{~T}$ |     6.0      |  +   |
|                    3                    | $\mathrm{~T}$ | $\mathrm{~F}$ |     5.0      |  -   |
|                    4                    | $\mathrm{~F}$ | $\mathrm{~F}$ |     4.0      |  +   |
|                    5                    | $\mathrm{~F}$ | $\mathrm{~T}$ |     7.0      |  -   |
|                    6                    | $\mathrm{~F}$ | $\mathrm{~T}$ |     3.0      |  -   |
|                    7                    | $\mathrm{~F}$ | $\mathrm{~F}$ |     8.0      |  -   |
|                    8                    | $\mathrm{~T}$ | $\mathrm{~F}$ |     7.0      |  +   |
|                    9                    | $\mathrm{~F}$ | $\mathrm{~T}$ |     5.0      |  -   |
|                   10                    | $\mathrm{~F}$ | $\mathrm{~F}$ |     2.0      |  +   |

##### **4.1 整个训练样本关于类属性的摘是多少?**
解. 类别 + 的概率为 $p^{+}=\frac{5}{10}$, 类别 - 的概率为 $p^{-}=\frac{5}{10}$, 因此熵为

$$
\text{Ent}(D)=-p^{+} \log _2 p^{+}-p^{-} \log _2 p^{-}=1 \text {. }
$$
##### **4.2 数据集中 $\mathrm{A} 、 \mathrm{~B}$ 两个属性的信息增益各是多少?**

解.

$$
\begin{aligned}
\text{Gain}(D, A) & =\text{Ent}(D)-\sum _v \frac{\left|D^v\right|}{|D|} \text{Ent}\left(D^v\right) \\
& =1-\left(\frac{4}{10}\left(-\frac{3}{4} \log _2 \frac{3}{4}-\frac{1}{4} \log _2 \frac{1}{4}\right)+\frac{6}{10}\left(-\frac{4}{6} \log _2 \frac{4}{6}-\frac{2}{6} \log _2 \frac{2}{6}\right)\right) \\
& =0.125
\end{aligned}
$$

$$
\begin{aligned}
\text{Gain}(D, B) & =\text{Ent}(D)-\sum_v \frac{\left|D^v\right|}{|D|} \text{Ent}\left(D^v\right) \\
& =1-\left(\frac{5}{10}\left(-\frac{2}{5} \log _2 \frac{2}{5}-\frac{3}{5} \log _2 \frac{3}{5}\right)+\frac{5}{10}\left(-\frac{2}{5} \log _2 \frac{2}{5}-\frac{3}{5} \log _2 \frac{3}{5}\right)\right) \\
& =0.029
\end{aligned}
$$
##### **4.3 对于属性 C, 计算所有可能划分的信息增益?**

解. 可取划分点为 $\{1.5,2.5,3.5,4.5,5.5,6.5,7.5\}$, 然后对应划分信息增益为:

$$
\begin{gather}
\text{Gain}(D, C, 1.5)=1-\frac{9}{10}\left( -\frac{4}{9} \log _2 \frac{4}{9}-\frac{5}{9} \log _2 \frac{5}{9}\right) \approx 0.108 \\
\text{Gain}(D, C, 2.5)=1-\frac{8}{10}\left( -\frac{3}{8} \log _2 \frac{3}{8}-\frac{5}{8} \log _2 \frac{5}{8}\right) \approx 0.236 \\
\end{gather}
$$

$$
\begin{gather}
\text{Gain}(D, C, 3.5)=1-\left[ \frac{3}{10}\left( -\frac{2}{3} \log _2 \frac{2}{3}-\frac{1}{3} \log _2 \frac{1}{3}\right) +\frac{7}{10}\left( -\frac{3}{7} \log _2 \frac{3}{7}-\frac{4}{7} \log _2 \frac{4}{7}\right)\right] \approx 0.035 \\
\text{Gain}(D, C, 4.5)=1-\left[ \frac{4}{10}\left( -\frac{3}{4} \log _2 \frac{3}{4}-\frac{1}{4} \log _2 \frac{1}{4}\right) +\frac{6}{10}\left( -\frac{2}{6} \log _2 \frac{2}{6}-\frac{4}{6} \log _2 \frac{4}{6}\right)\right] \approx 0.125 \\
\text{Gain}(D, C, 5.5)=1-\left[ \frac{6}{10}\left( -\frac{3}{6} \log _2 \frac{3}{6}-\frac{3}{6} \log _2 \frac{3}{6}\right) +\frac{4}{10}\left( -\frac{2}{4} \log _2 \frac{2}{4}-\frac{2}{4} \log _2 \frac{2}{4}\right) \right] =0 \\
\text{Gain}(D, C, 6.5)=1-\left[ \frac{7}{10}\left( -\frac{4}{7} \log _2 \frac{4}{7}-\frac{3}{7} \log _2 \frac{3}{7}\right) +\frac{3}{10}\left( -\frac{1}{3} \log _2 \frac{1}{3}-\frac{2}{3} \log _2 \frac{2}{3}\right) \right] \approx 0.035 \\
\text{Gain}(D, C, 7.5)=1-\frac{9}{10}\left( -\frac{5}{9} \log _2 \frac{5}{9}-\frac{4}{9} \log _2 \frac{4}{9}\right) \approx 0.108
\end{gather}
$$
##### **4.4 根据 Gini 指数, $A$ 和 $B$ 两个属性哪个是最优划分?**

解.

$$
\begin{gather}
\text {Gini}\_\text{index}(D, A) & =\frac{4}{10}\left(1-\left(\frac{3}{4}\right)^2-\left(\frac{1}{4}\right)^2\right)+\frac{6}{10}\left(1-\left(\frac{2}{6}\right)^2-\left(\frac{4}{6}\right)^2\right)=0.417 \\
\text {Gini}\_\text{index}(D, B) & =\frac{5}{10}\left(1-\left(\frac{2}{5}\right)^2-\left(\frac{3}{5}\right)^2\right)+\frac{5}{10}\left(1-\left(\frac{3}{5}\right)^2-\left(\frac{2}{5}\right)^2\right) =0.48
\end{gather}
$$

因此， $A$ 是最优划分。
##### **4.5 采用算法 C4.5，构造决策树。**

解. - 指标：信息增益率, 不是信息增益
- 构造方法和构造结果不唯一, 建议大家构造前简述自己的构造方法, 可以拿一半的过程分。
初始: $D=\{1,2,3,4,5,6,7,8,9,10\}$
第一层划分:

$$
\text{Gain}\_\text{ratio}(D, A)=\frac{\text{Gain}(D, A)}{\text{IV}(D, A)}=\frac{0.125}{0.971}=0.129
$$

$$
\begin{gathered}
\text {Gain}\_\text{ratio}(D, B)=\frac{\text{Gain}(D, B)}{\text{IV}(D, B)}=\frac{0.029}{1}=0.029 \\
\text{Gain}\_ \text{ratio~}(D, C, 2.5)=\frac{\text{Gain}(D, C, 2.5)}{\text{IV}(D, C, 2.5)}=\frac{0.236}{0.722}=0.326
\end{gathered}
$$

选择 $(C, 2.5)$ 作为 $D$ 的划分, 得到 $D_{c \leq 2.5}^1=\{1,10\}, D_{c>2.5}^1=\{2,3,4,5,7,8,9\}$ 。第二层划分: 因为 $D_{c \leq 2.5}^1$ 元素类别一致, 不再划分, 主要针对 $D_{c>2.5}^1$ 继续划分。

$$
\begin{gather}
\text {Gain} _\text{ratio}\left(D _{c\gt 2.5}^1, A\right)=\frac{\text{Gain}\left(D _{c\gt 2.5}^1, A\right)}{\text{IV}\left(D _{c\gt 2.5}^1, A\right)}=\frac{0.159}{0.954}=0.167 \\
\text {Gain} _\text{ratio}\left(D _{c\gt 2.5}^1, B\right)=\frac{\text{Gain}\left(D _{c\gt 2.5}^1, B\right)}{\text{IV}\left(D _{c\gt 2.5}^1, B\right)}=\frac{0.049}{1}=0.049 \\
\text {Gain} _\text{ratio}\left(D _{c\gt 2.5}^1, C, 3.5\right)=\frac{\text{Gain}\left(D _{c\gt 2.5}^1, C, 3.5\right)}{\text{IV}\left(D _{c\gt 2.5}^1, C, 3.5\right)}=\frac{0.092}{0.544}=0.169
\end{gather}
$$

选择 $(C, 3.5)$ 作为 $D_{c\gt 2.5}^1$ 的划分, 得到 $D_{2.5 \lt c \leq 3.5}^2=\{6\}, D_{c \geq 3.5}^2=\{2,3,4,5,7,8,9\}$ 。

第三层划分: 以此类推...

![image-20231022145317109](HW3&4%E8%A7%A3%E7%AD%94.assets/image-20231022145317109.png)
