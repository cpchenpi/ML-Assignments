{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run some setup code for this notebook.\n",
    "import pandas as pd\n",
    "\n",
    "# Some more magic so that the notebook will reload external python modules;\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    df = pd.read_json(path)\n",
    "    df_expanded = df[\"user\"].apply(lambda x: pd.Series(x))\n",
    "    df = pd.concat([df.drop(\"user\", axis=1), df_expanded], axis=1)\n",
    "    return df\n",
    "\n",
    "\n",
    "df = load_data(\"./data/train.json\")\n",
    "df2fill = load_data(\"./data/test.json\")\n",
    "\n",
    "print(df.shape, df2fill.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 处理自然语言信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel, BertConfig\n",
    "import numpy as np\n",
    "\n",
    "MODEL_PATH = \"D:/models/bert-case-based/\"\n",
    "\n",
    "config = BertConfig.from_pretrained(MODEL_PATH)\n",
    "tokenizer = BertTokenizer.from_pretrained(MODEL_PATH)\n",
    "model = BertModel.from_pretrained(MODEL_PATH)\n",
    "\n",
    "\n",
    "def str_embedding(s: str, max_len: int):\n",
    "    text_dict = tokenizer(\n",
    "        s,\n",
    "        max_length=max_len,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    return pd.Series(\n",
    "        model(text_dict[\"input_ids\"], attention_mask=text_dict[\"attention_mask\"])[0]\n",
    "        .detach()\n",
    "        .squeeze(0)\n",
    "        .numpy()\n",
    "        .flatten()\n",
    "    )\n",
    "\n",
    "def str_entropy(s: str):\n",
    "    from collections import Counter\n",
    "    n = len(s)\n",
    "    cnt = Counter(s)\n",
    "    res = 0.0\n",
    "    for _ch, c in cnt.items():\n",
    "        p = c / n\n",
    "        res -= p * np.log2(p)\n",
    "    return res\n",
    "\n",
    "\n",
    "arg_map = {\n",
    "    \"name\": (-1, -1),\n",
    "    \"screen_name\": (-1, -1),\n",
    "    \"location\": (5, 5),\n",
    "    \"description\": (25, 7),\n",
    "}\n",
    "\n",
    "\n",
    "def process_str(d: pd.DataFrame):\n",
    "    print(d.shape)\n",
    "    for name, (mxlen, pca_dim) in arg_map.items():\n",
    "        dlen = pd.DataFrame(d[name].apply(len))\n",
    "        dlen.columns += \"_len\"\n",
    "        d = pd.concat([d, dlen], axis=1)\n",
    "\n",
    "        dent = pd.DataFrame(d[name].apply(str_entropy))\n",
    "        dent.columns += \"_ent\"\n",
    "        d = pd.concat([d, dent], axis=1)\n",
    "\n",
    "        if mxlen != -1:\n",
    "            de = d[name].apply(str_embedding, args=(mxlen,))\n",
    "            from sklearn.decomposition import PCA\n",
    "\n",
    "            pca = PCA(n_components=pca_dim)\n",
    "            de = pd.DataFrame(pca.fit_transform(de))\n",
    "            de.columns = list(map(lambda x: name + str(x), range(pca_dim)))\n",
    "            d = pd.concat([d, de], axis=1)\n",
    "        print(d.shape)\n",
    "    return d\n",
    "\n",
    "\n",
    "def process2str(a: pd.DataFrame, b: pd.DataFrame):\n",
    "    N = len(a)\n",
    "    con = pd.concat([a, b])\n",
    "    con = con.reset_index(drop=True)\n",
    "    res = process_str(con)\n",
    "    return res[:N], res[N:]\n",
    "\n",
    "\n",
    "df, df2fill = process2str(df, df2fill)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(d: pd.DataFrame):\n",
    "    d.drop([\"id\", \"id_str\", \"utc_offset\", \"time_zone\"], axis=1, inplace=True)\n",
    "\n",
    "    d.drop(\n",
    "        [\n",
    "            \"name\",\n",
    "            \"screen_name\",\n",
    "            \"location\",\n",
    "            \"description\",\n",
    "            \"url\",\n",
    "            \"entities\",\n",
    "            \"profile_background_image_url\",\n",
    "            \"profile_background_image_url_https\",\n",
    "            \"profile_image_url\",\n",
    "            \"profile_image_url_https\",\n",
    "            \"profile_banner_url\",\n",
    "        ],\n",
    "        axis=1,\n",
    "        inplace=True,\n",
    "    )\n",
    "\n",
    "    old_columns = list(d.columns[1:])\n",
    "    d.columns = [\"created_at0\"] + old_columns\n",
    "    d.drop([\"created_at0\"], axis=1, inplace=True)\n",
    "    d[\"created_at\"] = pd.to_datetime(d[\"created_at\"], infer_datetime_format=True)\n",
    "    d[\"created_at\"] = d[\"created_at\"].apply(\n",
    "        lambda x: x.value // (10**9) / (24 * 60 * 60)\n",
    "    )\n",
    "\n",
    "    def col2rgb(s: str):\n",
    "        x = int(s, base=16)\n",
    "        return pd.Series([x // (256 * 256), (x // 256) % 256, x % 256])\n",
    "\n",
    "    for name in [\n",
    "        \"profile_background_color\",\n",
    "        \"profile_link_color\",\n",
    "        \"profile_sidebar_border_color\",\n",
    "        \"profile_sidebar_fill_color\",\n",
    "        \"profile_text_color\",\n",
    "    ]:\n",
    "        d_rgb = d[name].apply(col2rgb)\n",
    "        d_rgb.columns = [name + \"_r\", name + \"_g\", name + \"_b\"]\n",
    "        d = pd.concat([d.drop([name], axis=1), d_rgb], axis=1)\n",
    "\n",
    "    d[\"lang\"] = d[\"lang\"].apply(str.lower)\n",
    "\n",
    "    return d\n",
    "\n",
    "\n",
    "df_label = pd.get_dummies(df[\"label\"])[\"human\"]\n",
    "df.drop([\"label\"], axis=1, inplace=True)\n",
    "df2fill.drop([\"label\"], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "def dummy2(a: pd.DataFrame, b: pd.DataFrame):\n",
    "    N = len(a)\n",
    "    concat_dummy = pd.get_dummies(pd.concat([a, b]))\n",
    "    return concat_dummy[:N], concat_dummy[N:]\n",
    "\n",
    "\n",
    "df = preprocess(df)\n",
    "df2fill = preprocess(df2fill)\n",
    "\n",
    "df, df2fill = dummy2(df, df2fill)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.info())\n",
    "print(df2fill.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据探索"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "continuous_attrs = [\n",
    "    \"created_at\",\n",
    "    \"followers_count\",\n",
    "    \"friends_count\",\n",
    "    \"listed_count\",\n",
    "    \"favourites_count\",\n",
    "    \"statuses_count\",\n",
    "]\n",
    "\n",
    "df[continuous_attrs].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in continuous_attrs:\n",
    "    df[s] = np.log10(1 + df[s])\n",
    "    df2fill[s] = np.log10(1 + df2fill[s])\n",
    "\n",
    "for s in [\"statuses_count\"]:\n",
    "    df[s] = np.square(df[s])\n",
    "    df2fill[s] = np.square(df2fill[s])\n",
    "\n",
    "for s in [\"favourites_count\"]:\n",
    "    df[s] = np.power(df[s], 1.35)\n",
    "    df2fill[s] = np.power(df2fill[s], 1.35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axs_hist = plt.subplots(2, 3, figsize=(12, 8))\n",
    "\n",
    "for i, s in enumerate(continuous_attrs):\n",
    "    sns.histplot(data=df, x=s, hue=df_label, kde=True, ax=axs_hist[i // 3, i % 3])\n",
    "\n",
    "plt.savefig(\"image/hist.png\", dpi=300, bbox_inches=\"tight\", facecolor=\"white\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7, 5))\n",
    "sns.heatmap(df[continuous_attrs].corr(), annot=True)\n",
    "plt.savefig(\"image/heatmap.png\", dpi=300, bbox_inches=\"tight\", facecolor=\"white\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_label = np.array(df_label)\n",
    "global X_train, X_test, y_train, y_test\n",
    "global df2fill_scaled\n",
    "\n",
    "\n",
    "def make_data():\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    global X_train, X_test, y_train, y_test\n",
    "    global df2fill_scaled\n",
    "\n",
    "    X_train, X_test, y_train, y_test = map(np.array, train_test_split(df, df_label, test_size=0.2))\n",
    "\n",
    "    from sklearn.covariance import EllipticEnvelope\n",
    "    detector = EllipticEnvelope(contamination=0.05)\n",
    "    detector.fit(X_train)\n",
    "    pred = detector.predict(X_train)\n",
    "    X_train[pred==-1, :] = np.nan\n",
    "\n",
    "    from sklearn.impute import KNNImputer\n",
    "    imputer = KNNImputer()\n",
    "    X_train = imputer.fit_transform(X_train)\n",
    "\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    df2fill_scaled = scaler.transform(df2fill.values)\n",
    "\n",
    "\n",
    "make_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 逻辑回归"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LR_solve():\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "    LR_model = LogisticRegression(solver=\"saga\", max_iter=1000)\n",
    "    LR_model.fit(X_train, y_train)\n",
    "    LR_acc = LR_model.score(X_test, y_test)\n",
    "    return LR_acc\n",
    "\n",
    "\n",
    "LR_solve()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 支持向量机"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSVC_solve():\n",
    "    from sklearn.svm import LinearSVC\n",
    "\n",
    "    LSVC_model = LinearSVC(max_iter=2000)\n",
    "    LSVC_model.fit(X_train, y_train)\n",
    "    LSVC_acc = LSVC_model.score(X_test, y_test)\n",
    "    return LSVC_acc\n",
    "\n",
    "\n",
    "LSVC_solve()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 随机森林"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RF_solve():\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "    RF_model = RandomForestClassifier(n_estimators=256)\n",
    "    RF_model.fit(X_train, y_train)\n",
    "    RF_acc = RF_model.score(X_test, y_test)\n",
    "    return RF_acc\n",
    "\n",
    "\n",
    "RF_solve()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 梯度提升树"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GB_solve():\n",
    "    from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "    GB_model = GradientBoostingClassifier(n_estimators=256)\n",
    "    GB_model.fit(X_train, y_train)\n",
    "    GB_acc = GB_model.score(X_test, y_test)\n",
    "    return GB_acc\n",
    "\n",
    "\n",
    "GB_solve()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型比较"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "methods = {\n",
    "    \"Logistic Regression\": LR_solve,\n",
    "    \"Linear SVC\": LSVC_solve,\n",
    "    \"Random Forest\": RF_solve,\n",
    "    \"Gradient Boosting\": GB_solve,\n",
    "}\n",
    "accs = {name: [] for name in methods.keys()}\n",
    "\n",
    "for iter in range(5):\n",
    "    make_data()\n",
    "    for name, func in methods.items():\n",
    "        accs[name].append(func())\n",
    "        print(accs[name][-1], end=\" \")\n",
    "    print()\n",
    "\n",
    "compare = pd.DataFrame({\"Model\": [], \"Accuracy\": []})\n",
    "for name in methods.keys():\n",
    "    compare.loc[len(compare)] = [name, np.average(accs[name]) * 100]\n",
    "\n",
    "compare.sort_values(by=\"Accuracy\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型验证"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "make_data()\n",
    "RF_model = RandomForestClassifier(n_estimators=256)\n",
    "RF_model.fit(X_train, y_train)\n",
    "print(RF_model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = RF_model.predict(df2fill_scaled)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"./data/rawtest.json\", \"r\") as file:\n",
    "    data = json.load(file)\n",
    "    for i in range(len(data)):\n",
    "        data[i][\"label\"] = \"human\" if pred[i] else \"bot\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"./data/test.json\", \"w\") as file:\n",
    "#     json.dump(data, file, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
